# Dataset configuration
dataset:
  name: "wikitext"
  config_name: "wikitext-103-raw-v1"
  block_size: 128
  num_proc: 4
  use_subset: true
  eval_from_train: true  # Use a slice of the training set for validation
  subset_size:
    train: 20       # Total samples to overfit on.
    validation: 5   # Samples from the train set to use for validation.

# Model configuration
model:
  n_positions: 128
  n_embd: 256
  n_layer: 4
  n_head: 4

# Training configuration
training:
  output_dir: "./gpt2-overfit-test"
  run_name: "gpt2-overfit-test"
  overwrite_output_dir: true
  learning_rate: 0.00005
  num_train_epochs: 100 # High number of epochs to force overfit
  per_device_train_batch_size: 5
  per_device_eval_batch_size: 5
  eval_steps: 4      # Evaluate every epoch (20 samples / 5 batch_size = 4 steps)
  save_steps: 40     # Save every 10 epochs
  warmup_steps: 10
  do_eval: true
  logging_dir: './logs-overfit'
  logging_steps: 1 # Log every step to see loss plummet
  report_to: "wandb"

# Tokenizer configuration
tokenizer:
  vocab_file: "./wikitext-tokenizer-vocab.json"
  merges_file: "./wikitext-tokenizer-merges.txt"
