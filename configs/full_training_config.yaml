# Dataset configuration
dataset:
  name: "wikitext"
  config_name: "wikitext-103-raw-v1"
  block_size: 512
  num_proc: 4
  use_subset: false

# Model configuration
model:
  n_positions: 512
  n_embd: 768
  n_layer: 12
  n_head: 12

# Training configuration
training:
  output_dir: "./gpt2-wikitext-full"
  run_name: "gpt2-full-training"
  overwrite_output_dir: true
  learning_rate: 0.0005
  num_train_epochs: 5
  per_device_train_batch_size: 64 # Optimized for H100 GPU memory
  per_device_eval_batch_size: 64
  fp16: true # Enable mixed-precision for H100 performance
  eval_steps: 50
  save_steps: 1000
  logging_steps: 50
  warmup_steps: 500
  do_eval: true
  logging_dir: './logs-full'
  report_to: "wandb"
  torch_compile: true # Enable PyTorch 2.0 compilation
  optim: "adamw_torch_fused" # Use a faster, fused optimizer
  dataloader_num_workers: 4 # Parallelize data loading

# Tokenizer configuration
tokenizer:
  vocab_file: "./gpt2-tokenizer-vocab.json"
  merges_file: "./gpt2-tokenizer-merges.txt"

environment_variables:
  HF_HUB_ENABLE_HF_TRANSFER: 0
